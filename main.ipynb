{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46c2472cae52391",
   "metadata": {},
   "source": [
    "# Project: Herseninfarct\n",
    "### Team: Undefined\n",
    "### Teamleden:\n",
    "- **Sebastiaan Westerlaken**\n",
    "- **Michal Kakol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:02.448643Z",
     "start_time": "2025-10-05T14:33:01.860716Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "# Import src\n",
    "from src.preprocessing import clean_data, normalize_data\n",
    "from src.eda import eda_sum, impossible_values\n",
    "from src.model import train_knn, train_lr, train_svm, train_decision_tree, train_random_forest, train_gradient_boosting, train_xgboost, train_custom_ensemble\n",
    "from src.other_functions import evaluate_knn, export_submission\n",
    "\n",
    "# Data\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_example = pd.read_csv(\"data/sample_submission.csv\")\n",
    "df_submission = pd.DataFrame(columns=[\"id\", \"stroke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f930f4",
   "metadata": {},
   "source": [
    "## Makkelijk overzicht data\n",
    "\n",
    "| **Kolomnaam**                    | **Beschrijving**                                                                                |\n",
    "| -------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `id`                             | Unieke identificatiecode voor elke persoon in de dataset.                                       |\n",
    "| `age`                            | Leeftijd van de persoon (in jaren).                                                             |\n",
    "| `hypertension`                   | Of de persoon een hoge bloeddruk (hypertensie) heeft (1 = Ja, 0 = Nee).                         |\n",
    "| `heart_disease`                  | Of de persoon een hartaandoening heeft (1 = Ja, 0 = Nee).                                       |\n",
    "| `avg_glucose_level`              | Gemiddeld glucosegehalte in het bloed (in mg/dL).                                               |\n",
    "| `bmi`                            | Body Mass Index (BMI) — maat voor vetpercentage op basis van lengte en gewicht.                 |\n",
    "| `gender_Female`                  | True als de persoon **vrouw** is; anders False.                                                 |\n",
    "| `gender_Male`                    | True als de persoon **man** is; anders False.                                                   |\n",
    "| `gender_Other`                   | True als de persoon een **ander gender** heeft; anders False.                                   |\n",
    "| `ever_married_No`                | True als de persoon **nog nooit getrouwd** is; anders False.                                    |\n",
    "| `ever_married_Yes`               | True als de persoon **ooit getrouwd** is; anders False.                                         |\n",
    "| `work_type_Govt_job`             | True als de persoon een **overheidsbaan** heeft; anders False.                                  |\n",
    "| `work_type_Never_worked`         | True als de persoon **nog nooit gewerkt** heeft; anders False.                                  |\n",
    "| `work_type_Private`              | True als de persoon in de **private sector** werkt; anders False.                               |\n",
    "| `work_type_Self-employed`        | True als de persoon **zelfstandig ondernemer** is; anders False.                                |\n",
    "| `work_type_children`             | True als de persoon een **kind** is (nog geen onderdeel van de beroepsbevolking); anders False. |\n",
    "| `Residence_type_Rural`           | True als de persoon in een **landelijk** gebied woont; anders False.                            |\n",
    "| `Residence_type_Urban`           | True als de persoon in een **stedelijk** gebied woont; anders False.                            |\n",
    "| `smoking_status_formerly smoked` | True als de persoon **vroeger rookte**; anders False.                                           |\n",
    "| `smoking_status_never smoked`    | True als de persoon **nooit gerookt** heeft; anders False.                                      |\n",
    "| `smoking_status_smokes`          | True als de persoon **momenteel rookt**; anders False.                                          |\n",
    "| `stroke`                         | Doelvariabele — of de persoon ooit een **beroerte (stroke)** heeft gehad (1 = Ja, 0 = Nee).     |\n",
    "\n",
    "*(ChatGPT, 2025, prompt 1: Markdown Table Help)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed7aec29b65d10",
   "metadata": {},
   "source": [
    "# 1. Exploratieve Data Analyse<br>\n",
    "\n",
    "Hieronder voeren wij eerst de exploratieve data analyse uit die wordt aangeroepen vanuit het src.eda bestand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a7d989d469af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:29:40.241466Z",
     "start_time": "2025-10-05T14:29:40.225211Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_sum(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4c6a6d8c6ee75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploratieve Data Analyse\n",
    "\n",
    "### Beoordeling van de dataset\n",
    "\n",
    "De dataset kan direct gebruikt worden met Scikit-Learn. Hij is ingeladen als een DataFrame, er zijn geen ontbrekende waarden en alle kolommen zijn numeriek. Het gaat om een combinatie van booleans, integers en floats.\n",
    "\n",
    "Wat betreft de meetniveaus:\n",
    "- Nominaal: heart_disease, hypertension, work_type_Self-employed, stroke\n",
    "- Ordinaal: geen\n",
    "- Interval: geen\n",
    "- Ratio: age, avg_glucose_level, bmi\n",
    "\n",
    "Hoewel sommige nominale variabelen als integers zijn opgeslagen, zijn ze binair van aard. Voor machine learning hoeft hier niets aan veranderd te worden.\n",
    "\n",
    "---\n",
    "\n",
    "### Correlaties en belangrijkste variabelen\n",
    "\n",
    "Om te kijken naar mogelijke multicollineariteit en te zien welke features het meest relevant zijn voor de target (stroke), is een correlatie-analyse gedaan.\n",
    "\n",
    "Sterkst gecorreleerde variabelen met stroke zijn:\n",
    "- age 0.149\n",
    "- heart_disease 0.105\n",
    "- hypertension 0.084\n",
    "- avg_glucose_level 0.077\n",
    "- work_type_Self-employed 0.062\n",
    "- smoking_status_formerly smoked 0.034\n",
    "- bmi 0.021\n",
    "\n",
    "Geen enkele correlatie is hoger dan 0.2, dus alle features kunnen worden behouden voor modelbouw.\n",
    "\n",
    "---\n",
    "\n",
    "### Belangrijkste observaties\n",
    "\n",
    "Leeftijd blijkt duidelijk een belangrijke factor: jongere mensen hebben bijna geen kans op een beroerte, het risico neemt vooral toe na 35 jaar. BMI lijkt minder invloed te hebben dan verwacht. Voor roken valt op dat voormalige rokers iets meer risico lijken te hebben dan huidige rokers. Het gemiddeld glucosegehalte ligt hoger bij mensen met een beroerte, maar strokes komen voor bij een breed bereik aan waarden.\n",
    "\n",
    "---\n",
    "\n",
    "### Statistische kenmerken\n",
    "\n",
    "Voor de niet-boolean variabelen zijn enkele kernstatistieken berekend. Age heeft een standaarddeviatie van 22.48 en is bijna symmetrisch verdeeld, met een licht afgeplatte verdeling. Avg_glucose_level ligt gemiddeld rond 104 met een standaarddeviatie van 42, de verdeling is rechts-scheef door enkele uitschieters. Deze uitschieters kunnen relevant zijn voor het model.\n",
    "\n",
    "---\n",
    "\n",
    "### Klasse-ongelijkheid\n",
    "\n",
    "De dataset is sterk uit balans, met slechts 517 positieve stroke-gevallen. Daarom worden bij het evalueren van modellen vooral de F1-score en de confusion matrix gebruikt, omdat alleen accuracy niet genoeg zegt over de prestaties op de minderheidsklasse.\n",
    "\n",
    "---\n",
    "\n",
    "### Samenvatting\n",
    "\n",
    "De dataset is geschikt voor machine learning. Er zijn geen missende waarden, geen sterke correlaties en de belangrijkste voorspellende variabelen lijken age, heart_disease, hypertension en avg_glucose_level. Vanwege de sterke class imbalance moet bij modeltraining extra aandacht worden besteed aan geschikte evaluatiemetrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb19032",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Data opschonen en standaardiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de445b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:31.125223Z",
     "start_time": "2025-10-05T13:59:31.084152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>ever_married_No</th>\n",
       "      <th>...</th>\n",
       "      <th>work_type_Govt_job</th>\n",
       "      <th>work_type_Never_worked</th>\n",
       "      <th>work_type_Private</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>Residence_type_Rural</th>\n",
       "      <th>Residence_type_Urban</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21761</th>\n",
       "      <td>999</td>\n",
       "      <td>-0.924639</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.633968</td>\n",
       "      <td>1.460447</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22172</th>\n",
       "      <td>38471</td>\n",
       "      <td>0.277480</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>0.626586</td>\n",
       "      <td>0.365047</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31165</th>\n",
       "      <td>8123</td>\n",
       "      <td>-0.746547</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.653917</td>\n",
       "      <td>-0.954713</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22405</th>\n",
       "      <td>51151</td>\n",
       "      <td>-0.256795</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.684552</td>\n",
       "      <td>0.048304</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>15974</td>\n",
       "      <td>1.212462</td>\n",
       "      <td>-0.311805</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>2.293235</td>\n",
       "      <td>1.447250</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       age  hypertension  heart_disease  avg_glucose_level  \\\n",
       "21761    999 -0.924639     -0.311805      -0.212204          -0.633968   \n",
       "22172  38471  0.277480     -0.311805      -0.212204           0.626586   \n",
       "31165   8123 -0.746547     -0.311805      -0.212204          -0.653917   \n",
       "22405  51151 -0.256795     -0.311805      -0.212204          -0.684552   \n",
       "7613   15974  1.212462     -0.311805      -0.212204           2.293235   \n",
       "\n",
       "            bmi  gender_Female  gender_Male  gender_Other  ever_married_No  \\\n",
       "21761  1.460447          False         True         False             True   \n",
       "22172  0.365047          False         True         False            False   \n",
       "31165 -0.954713           True        False         False             True   \n",
       "22405  0.048304          False         True         False            False   \n",
       "7613   1.447250           True        False         False            False   \n",
       "\n",
       "       ...  work_type_Govt_job  work_type_Never_worked  work_type_Private  \\\n",
       "21761  ...                True                   False              False   \n",
       "22172  ...               False                   False              False   \n",
       "31165  ...                True                   False              False   \n",
       "22405  ...               False                   False              False   \n",
       "7613   ...               False                   False              False   \n",
       "\n",
       "       work_type_Self-employed  work_type_children  Residence_type_Rural  \\\n",
       "21761                    False               False                 False   \n",
       "22172                     True               False                 False   \n",
       "31165                    False               False                 False   \n",
       "22405                     True               False                  True   \n",
       "7613                      True               False                 False   \n",
       "\n",
       "       Residence_type_Urban  smoking_status_formerly smoked  \\\n",
       "21761                  True                           False   \n",
       "22172                  True                           False   \n",
       "31165                  True                           False   \n",
       "22405                 False                           False   \n",
       "7613                   True                           False   \n",
       "\n",
       "       smoking_status_never smoked  smoking_status_smokes  \n",
       "21761                        False                  False  \n",
       "22172                        False                  False  \n",
       "31165                         True                  False  \n",
       "22405                         True                  False  \n",
       "7613                          True                  False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opschonen\n",
    "df_train_clean = clean_data(df_train)\n",
    "\n",
    "# Features + target\n",
    "X = df_train_clean.drop(columns=['stroke'])\n",
    "y = df_train_clean['stroke']\n",
    "\n",
    "# Splitsen in train / validatie\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standaardiseren en align train & test automatisch\n",
    "X_scaled_train, X_scaled_val, scaler = normalize_data(X_train, X_val)\n",
    "X_scaled_test, _, _ = normalize_data(X_train, df_test)\n",
    "X_scaled_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc156ab8",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699f4caf7fad99e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:34.440794Z",
     "start_time": "2025-10-05T13:59:33.945493Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_knn(X_scaled_train, X_scaled_val, y_train, y_val, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ffc11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluatie van het model\n",
    "\n",
    "Om het KNN-model te beoordelen gebruiken we enkele standaard metrics: accuracy, precision, recall en F1-score. Al deze waarden zijn af te leiden uit de confusion matrix, die laat zien hoe goed het model voorspellingen maakt.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Voor het model met n_neighbors gelijk aan 1 ziet de confusion matrix er als volgt uit:\n",
    "\n",
    "- Actual zonder stroke voorspeld als geen stroke: 6521\n",
    "- Actual zonder stroke voorspeld als stroke: 72\n",
    "- Actual met stroke voorspeld als geen stroke: 102\n",
    "- Actual met stroke voorspeld als stroke: 1\n",
    "\n",
    "In termen van de standaard terminologie betekent dit:\n",
    "- True Negatives: 6521\n",
    "- False Positives: 72\n",
    "- False Negatives: 102\n",
    "- True Positives: 1\n",
    "\n",
    "---\n",
    "\n",
    "### Uitleg van de metrics\n",
    "\n",
    "Accuracy meet het aantal correcte voorspellingen, in dit geval ongeveer 97 procent. Dit lijkt hoog, maar is misleidend door de scheve verdeling van de klassen.\n",
    "\n",
    "Precision laat zien hoe vaak de voorspelde positieve gevallen echt positief zijn. Voor dit model is dat maar 0.01, dus het model voorspelt veel te vaak een stroke terwijl dat niet klopt.\n",
    "\n",
    "Recall geeft aan hoeveel van de echte strokes het model herkent. Ook dit is 0.01, wat betekent dat bijna alle strokes gemist worden.\n",
    "\n",
    "De F1-score combineert precision en recall tot één waarde die beide aspecten meeweegt. Bij scheve datasets geeft dit een realistischer beeld van het model.\n",
    "\n",
    "---\n",
    "\n",
    "### Waarom F1-score belangrijk is\n",
    "\n",
    "De dataset bevat veel meer mensen zonder stroke dan met stroke. Bij zulke datasets kan een model dat altijd “geen stroke” voorspelt toch een hoge accuracy halen. Daarom is de F1-score hier een betere maatstaf. Het laat zien hoe goed het model de kleine positieve klasse oppikt en geeft een eerlijker beeld van de prestaties bij het detecteren van strokes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d3d57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Modelleren en uitleg modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d8e47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Hoe het model werkt:\n",
    "KNN is een niet-parametrisch classificatiemodel dat een nieuwe observatie indeelt op basis van de afstand tot de dichtstbijzijnde trainingspunten. Meestal wordt de Euclidische afstand gebruikt, wat de rechte lijn tussen twee punten meet:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_i (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "Een alternatief is de Manhattan afstand, die de afstand langs de assen berekent:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sum_i |p_i - q_i|\n",
    "$$\n",
    "\n",
    "Het model kiest de klasse van het nieuwe punt op basis van de meerderheid van de $k$ dichtstbijzijnde buren.\n",
    "\n",
    "---\n",
    "\n",
    "Waarom standaardisatie belangrijk is:\n",
    "Omdat KNN afstanden vergelijkt tussen features, moeten alle features op dezelfde schaal staan. Zonder standaardisatie kunnen features met grotere waarden, zoals inkomen, de afstand te veel domineren vergeleken met kleinere waarden zoals leeftijd. Dit kan de voorspellingen van het model vervormen.\n",
    "\n",
    "---\n",
    "\n",
    "Beste hyperparameters:\n",
    "- metric = euclidean\n",
    "- k = 11 \n",
    "- weights = distance\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.076.\n",
    "\n",
    "---\n",
    "\n",
    "Regularisatie bij KNN:<br>\n",
    "De parameter $k$ werkt bij dit model als een soort regularisatie:\n",
    "- Een kleine $k$ betekent dat het model sterk reageert op individuele datapunten en kan overfitten.\n",
    "- Een grotere $k$ maakt het model gladder en helpt bij generalisatie.\n",
    "\n",
    "Daarnaast zorgt weights = distance ervoor dat buren die dichterbij liggen meer invloed hebben op de voorspelling, wat zorgt voor een stabieler resultaat.<br>\n",
    "*(1.6. Nearest Neighbors, n.d.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9149aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:43.343890Z",
     "start_time": "2025-10-05T13:59:37.013228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train KNN\n",
    "knn_model, knn_params, knn_f1 = train_knn(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred = knn_model.predict(X_scaled_val)\n",
    "val_f1_knn = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation F1 score:\", val_f1_knn)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f23f4046cf494c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 2: Logistic Regression\n",
    "\n",
    "Hoe het model werkt:\n",
    "Logistische regressie voorspelt de kans dat een observatie tot klasse 1 behoort met behulp van de sigmoidfunctie:\n",
    "\n",
    "$$\n",
    "P(y=1 | x) = \\frac{1}{1 + e^{-(w^T x + b)}}\n",
    "$$\n",
    "\n",
    "Als deze kans groter is dan 0.5 voorspelt het model klasse 1, anders klasse 0.\n",
    "\n",
    "---\n",
    "\n",
    "Loss-functie:\n",
    "Om te meten hoe goed de voorspellingen zijn wordt de log-loss gebruikt, ook bekend als binaire cross-entropy:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent:\n",
    "Het model past de gewichten aan door de loss te minimaliseren met gradient descent:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Hierbij is α de learning rate. Bijvoorbeeld, als de gradient 4 is en α 0.1, wordt w aangepast met 0.4 richting het minimum van de loss.\n",
    "\n",
    "---\n",
    "\n",
    "Regularisatie en overfitting:\n",
    "Regularisatie helpt overfitting te voorkomen door grote coëfficiënten te straffen.\n",
    "\n",
    "- L1 (Lasso) kan sommige gewichten nul maken en werkt zo ook als feature selectie.\n",
    "- L2 (Ridge) maakt gewichten kleiner zonder ze nul te maken, waardoor het model beter generaliseert.\n",
    "\n",
    "---\n",
    "\n",
    "Beste hyperparameters:\n",
    "- C = 0.01\n",
    "- penalty = l1\n",
    "- solver = liblinear\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.125.<br>\n",
    "*(1.1. Linear Models, n.d.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5601e157a04add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:20:34.122393Z",
     "start_time": "2025-10-05T11:19:24.177904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model, lr_params, lr_f1 = train_lr(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred = lr_model.predict(X_scaled_val)\n",
    "val_f1_lr = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation F1 score:\", val_f1_lr)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5f9d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 3: Support Vector Machine (SVM)\n",
    "\n",
    "Hoe het model werkt:<br>\n",
    "SVM is een classificatiemodel dat een optimale scheidingslijn, of hypervlak, zoekt om de twee klassen van elkaar te scheiden met de grootst mogelijke marge:\n",
    "\n",
    "$$\n",
    "\\text{maximize } \\frac{2}{||w||} \\text{ onder de constraint } y_i (w^T x_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "Hierbij zijn:\n",
    "- ($w$) de gewichten, ($b$) de bias\n",
    "- ($x_i$) de featurevector van observatie ($i$)\n",
    "- ($y_i$ in ${-1, 1}$) de klasse van observatie ($i$)\n",
    "\n",
    "---\n",
    "\n",
    "Kernel en Kernel Trick:<br>\n",
    "Wanneer data niet lineair scheidbaar zijn, kan een kernel $K(x_i, x_j)$ de data transformeren naar een hogere dimensie:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
    "$$\n",
    "\n",
    "De kernel trick maakt het mogelijk $\\phi(x)$ niet expliciet te berekenen, waardoor het model efficiënter blijft. Bijvoorbeeld, met twee features $(x_1, x_2)$ en een linear kernel:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = x_i^T x_j = x_{i1} x_{j1} + x_{i2} x_{j2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Loss-functie:<br>\n",
    "SVM gebruikt hinge loss:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\max(0, 1 - y \\hat{y})\n",
    "$$\n",
    "\n",
    "Met $\\hat{y} = w^T x + b$ . Het verlies is nul als een voorbeeld correct is geclassificeerd met marge groter dan of gelijk aan 1, anders neemt het lineair toe.\n",
    "\n",
    "---\n",
    "\n",
    "Regularisatie:<br>\n",
    "De C-parameter reguleert de balans tussen correcte classificatie en maximale marge.\n",
    "- Hoger C betekent minder foutacceptatie\n",
    "- Lager C laat een grotere marge toe\n",
    "\n",
    "De totale loss met regularisatie kan worden weergegeven als:\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{1}{2} ||w||^2 + C \\sum_i \\max(0, 1 - y_i (w^T x_i + b))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Gradient / Optimalisatie:<br>\n",
    "Het model past de gewichten aan op een manier dat de som van de margin-fouten en de regularisatie zo klein mogelijk wordt.\n",
    "\n",
    "---\n",
    "\n",
    "Beste hyperparameters:\n",
    "- C = 0.1\n",
    "- kernel = linear\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.088.\n",
    "\n",
    "Voordeel / Nadeel:<br>\n",
    "SVM kan goed omgaan met hogere dimensionale data, maar RBF/poly kernels zijn traag bij grote datasets.<br>\n",
    "*(1.4. Support Vector Machines, n.d.)*<br>\n",
    "*(Wikipedia contributors, 2025)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa653e6fef813da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:42:43.275355Z",
     "start_time": "2025-10-05T11:20:34.322981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "svm_model, svm_params, svm_f1 = train_svm(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred = svm_model.predict(X_scaled_val)\n",
    "val_f1_svm = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation F1 score:\", val_f1_svm)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65ed59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 4: Decision Tree\n",
    "\n",
    "Hoe het model werkt:\n",
    "Decision Tree splitst de data op basis van beslissingen over features. Elke splitsing verdeelt de data in twee takken totdat een stopcriterium is bereikt, zoals een maximale diepte of minimaal aantal voorbeelden in een blad.\n",
    "\n",
    "---\n",
    "\n",
    "Splitsingscriteria:\n",
    "De splitsing wordt gekozen op basis van het criterium dat de impurity of onzekerheid het meest vermindert. Twee veelgebruikte criteria zijn:\n",
    "\n",
    "- Gini impurity:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "- Entropy of Information Gain:\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Hierbij is $(p_i)$ de kans dat een voorbeeld tot klasse $(i)$ behoort.\n",
    "\n",
    "---\n",
    "\n",
    "Regularisatie en Pruning:\n",
    "Om overfitting te voorkomen kunnen verschillende parameters worden gebruikt:\n",
    "- max_depth: maximale boomdiepte\n",
    "- min_samples_split: minimaal aantal voorbeelden nodig om een knoop te splitsen\n",
    "- min_samples_leaf: minimaal aantal voorbeelden per blad\n",
    "- Pruning: takken die weinig bijdragen worden verwijderd\n",
    "\n",
    "Pruning haalt takken weg die weinig bijdragen; dit helpt overfitting voorkomen, maar als je te veel weghaalt kan het model te simpel worden.\n",
    "\n",
    "---\n",
    "\n",
    "Beste hyperparameters:\n",
    "- criterion: entropy\n",
    "- max_depth: 5\n",
    "- min_samples_split: 1\n",
    "- min_samples_leaf: 2\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.0997.\n",
    "\n",
    "Voordeel:<br>\n",
    "Decision Trees zijn makkelijk te visualiseren en te begrijpen.<br>\n",
    "*(1.10. Decision Trees, n.d.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da03b5e125ae635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:43:07.006607Z",
     "start_time": "2025-10-05T11:42:43.456644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train DT\n",
    "dt_model, dt_params, dt_f1 = train_decision_tree(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred = dt_model.predict(X_scaled_val)\n",
    "val_f1_dt = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation F1 score:\", val_f1_dt)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f62468",
   "metadata": {},
   "source": [
    "## Model 5: Ensembles: Random Forest, Gradient Boosted Trees en XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "Bagging vs Boosting:\n",
    "> Bagging (Bootstrap Aggregating) traint meerdere modellen onafhankelijk op verschillende subsets van de data. Dit vermindert de variance en voorkomt overfitting.<br>\n",
    "> Boosting traint modellen achter elkaar, waarbij elke nieuwe boom focust op de fouten van erdere bomen. Dit kan een betere prestate opleveren, maar is sequential en vaak dus trager.\n",
    "\n",
    "---\n",
    "\n",
    "Random Forest (RF):<br>\n",
    "Random Forest combineert meerdere beslisbomen via bagging, oftewel bootstrap aggregation. Elke boom wordt onafhankelijk getraind, waardoor training relatief snel kan verlopen en parallel uitgevoerd wordt.<br>\n",
    "Belangrijke hyperparameters zijn n_estimators, max_depth, min_samples_split, min_samples_leaf en criterion. \n",
    "De beste instellingen voor dit model zijn:\n",
    "- criterion = gini\n",
    "- max_depth = 5\n",
    "- min_samples_leaf = 1\n",
    "- min_samples_split = 2\n",
    "- n_estimators = 100\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.108.\n",
    "\n",
    "Voordeel / Nadeel:<br>\n",
    "parallel trainbaar en robuust, maar minder goed bij scheve datasets of zeldzame klassen.\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Boosted Trees (GBT):<br>\n",
    "GBT traint meerdere bomen achter elkaar met boosting. Elke nieuwe boom corrigeert de fouten van de vorige bomen. Dit geeft vaak betere prestaties, maar de training duurt langer omdat het sequential is.<br>\n",
    "Belangrijke hyperparameters zijn n_estimators, learning_rate, max_depth, min_samples_split en min_samples_leaf.<br>\n",
    "De beste instellingen zijn:<br> \n",
    "- learning_rate = 0.1\n",
    "- max_depth = 3\n",
    "- min_samples_leaf = 1\n",
    "- min_samples_split = 2\n",
    "- n_estimators = 200 \n",
    "\n",
    "Dit levert een F1-score van 0.106.\n",
    "\n",
    "Voordeel / Nadeel:<br>\n",
    "Vaak hoge acuraccy, maar training is sequential en daardoor vrij langzaam; gevoelig voor overfitting bij te diepe bomen.\n",
    "\n",
    "---\n",
    "\n",
    "XGBoost:<br>\n",
    "XGBoost is een geavanceerde vorm van boosting, die door optimalisaties zoals parallel split search sneller kan trainen. Extra hyperparameters zoals subsample en colsample_bytree helpen bij regularisatie en verminderen variance.<br> Belangrijke hyperparameters zijn n_estimators, learning_rate, max_depth, subsample en colsample_bytree.<br>\n",
    "De beste instellingen zijn:\n",
    "- colsample_bytree = 0.8\n",
    "- learning_rate = 0.01\n",
    "- max_depth = 3\n",
    "- n_estimators = 200\n",
    "- subsample = 0.8\n",
    "\n",
    "Dit model behaalt een F1-score van 0.106.<br>\n",
    "*(1.11. Ensembles: Gradient Boosting, Random Forests, Bagging, Voting, Stacking, n.d.)*<br>\n",
    "*(XGBoost Documentation — Xgboost 3.0.5 Documentation, n.d.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313ab4ab7adbddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:59:08.539550Z",
     "start_time": "2025-10-05T12:59:03.148797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train models\n",
    "rf_model, rf_params, rf_f1 = train_random_forest(X_scaled_train, y_train)\n",
    "gb_model, gb_params, gb_f1 = train_gradient_boosting(X_scaled_train, y_train)\n",
    "xg_model, xg_params, xg_f1 = train_xgboost(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred_rf = rf_model.predict(X_scaled_val)\n",
    "y_val_pred_gb = gb_model.predict(X_scaled_val)\n",
    "y_val_pred_xg = xg_model.predict(X_scaled_val)\n",
    "val_f1_rf = f1_score(y_val, y_val_pred_rf)\n",
    "val_f1_gb = f1_score(y_val, y_val_pred_gb)\n",
    "val_f1_xg = f1_score(y_val, y_val_pred_xg)\n",
    "\n",
    "print(\"Validation F1 score rf:\", val_f1_rf)\n",
    "print(classification_report(y_val, y_val_pred_rf))\n",
    "print(\"Validation F1 score gb:\", val_f1_gb)\n",
    "print(classification_report(y_val, y_val_pred_gb))\n",
    "print(\"Validation F1 score xg:\", val_f1_xg)\n",
    "print(classification_report(y_val, y_val_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16d04c6ac78245",
   "metadata": {},
   "source": [
    "## 3.6 Zelf samengesteld ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c78eb",
   "metadata": {},
   "source": [
    "### Custom Ensemble Model\n",
    "\n",
    "Werking van het ensemble:\n",
    "Het ensemble combineert minimaal drie verschillende modellen: KNN, Logistic Regression en Random Forest. Hierbij wordt soft voting gebruikt, wat betekent dat de voorspelde kansen van de basismodellen gemiddeld worden. De klasse met de hoogste gemiddelde kans wordt gekozen als uiteindelijke voorspelling.\n",
    "\n",
    "Keuze van modellen:\n",
    "- KNN: afstand-gebaseerd en gevoelig voor de schaal van features.\n",
    "- Logistic Regression: lineair en kans-gebaseerd.\n",
    "- Random Forest: non-lineair, robuust tegen outliers en kan interacties tussen features oppikken.\n",
    "\n",
    "Beste hyperparameters:\n",
    "- knn_n_neighbors = 9\n",
    "- knn_weights = distance\n",
    "- lr_C = 10\n",
    "- rf_max_depth = 5\n",
    "- rf_n_estimators = 50\n",
    "\n",
    "Met deze instellingen behaalt het ensemble een F1-score van 0.095.\n",
    "\n",
    "Waarom dit ensemble werkt:\n",
    "Door verschillende modellen te combineren verminderen we fouten die individuele modellen maken. Dit helpt vooral bij een dataset met veel meer negatieve dan positieve gevallen, omdat sommige modellen beter presteren bij het herkennen van de zeldzame positieve klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7a7ccaf0ffa7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:28:58.491213Z",
     "start_time": "2025-10-05T13:59:49.349786Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m custom_model, custom_params, custom_f1 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_custom_ensemble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_scaled_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Evaluatie\u001B[39;00m\n\u001B[1;32m      5\u001B[0m y_val_pred \u001B[38;5;241m=\u001B[39m custom_model\u001B[38;5;241m.\u001B[39mpredict(X_scaled_val)\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/src/model.py:355\u001B[0m, in \u001B[0;36mtrain_custom_ensemble\u001B[0;34m(X, y)\u001B[0m\n\u001B[1;32m    345\u001B[0m scorer \u001B[38;5;241m=\u001B[39m make_scorer(f1_score)\n\u001B[1;32m    347\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[1;32m    348\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mpipeline,\n\u001B[1;32m    349\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    353\u001B[0m )\n\u001B[0;32m--> 355\u001B[0m \u001B[43mgrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    357\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m    358\u001B[0m best_params \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/sklearn/base.py:1365\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1358\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1360\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1361\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1362\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1363\u001B[0m     )\n\u001B[1;32m   1364\u001B[0m ):\n\u001B[0;32m-> 1365\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1045\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1046\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1047\u001B[0m     )\n\u001B[1;32m   1049\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1051\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1605\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1604\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1605\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:997\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    993\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    994\u001B[0m         )\n\u001B[1;32m    995\u001B[0m     )\n\u001B[0;32m--> 997\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    998\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    999\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1000\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1002\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1017\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1018\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1019\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1020\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:82\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     73\u001B[0m warning_filters \u001B[38;5;241m=\u001B[39m warnings\u001B[38;5;241m.\u001B[39mfilters\n\u001B[1;32m     74\u001B[0m iterable_with_config_and_warning_filters \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     75\u001B[0m     (\n\u001B[1;32m     76\u001B[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     81\u001B[0m )\n\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/joblib/parallel.py:2072\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2066\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2067\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2068\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2069\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2070\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/joblib/parallel.py:1682\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1679\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1681\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1682\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1684\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1685\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1687\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1688\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Repositories/ML/herseninfarct_ml/.venv/lib/python3.10/site-packages/joblib/parallel.py:1800\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_ordered:\n\u001B[1;32m   1790\u001B[0m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[1;32m   1791\u001B[0m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1795\u001B[0m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[1;32m   1796\u001B[0m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[1;32m   1797\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   1798\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING\n\u001B[1;32m   1799\u001B[0m     ):\n\u001B[0;32m-> 1800\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1801\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1804\u001B[0m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[1;32m   1805\u001B[0m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1811\u001B[0m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[1;32m   1812\u001B[0m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "custom_model, custom_params, custom_f1 = train_custom_ensemble(X_scaled_train, y_train)\n",
    "\n",
    "# Evaluatie\n",
    "y_val_pred = custom_model.predict(X_scaled_val)\n",
    "val_f1_custom = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation F1 score:\", val_f1_custom)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c40a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:20:00.659963Z",
     "start_time": "2025-10-05T12:19:56.783553Z"
    }
   },
   "outputs": [],
   "source": [
    "#Export\n",
    "# - ChatGPT, 2025, prompt 1: Export. https://chatgpt.com/share/68e262a7-8b80-800a-9b55-d7e59d1f731a\n",
    "test_ids = df_test['id']\n",
    "predictions = custom_model.predict(X_scaled_test)\n",
    "df_submission['id'] = test_ids\n",
    "df_submission['stroke'] = predictions\n",
    "\n",
    "df_submission.to_csv('data/submission_custom_ensemble.csv', index=False)\n",
    "#print(df_submission.describe())\n",
    "\n",
    "test_ids = df_test['id']\n",
    "\n",
    "trained_models = {\n",
    "    'knn': (knn_model, val_f1_knn),\n",
    "    'logreg': (lr_model, val_f1_lr),\n",
    "    'svm': (svm_model, val_f1_svm),\n",
    "    'decision_tree': (dt_model, val_f1_dt),\n",
    "    'random_forest': (rf_model, val_f1_rf),\n",
    "    'gradient_boosting': (gb_model, val_f1_gb),\n",
    "    'xgboost': (xg_model, val_f1_xg),\n",
    "    'custom_ensemble': (custom_model, val_f1_custom)\n",
    "}\n",
    "\n",
    "for name, (model, f1_val) in trained_models.items():\n",
    "    filename = f'data/submission_{name}_f1_{f1_val:.4f}.csv'\n",
    "    export_submission(model, X_scaled_test, test_ids, filename)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Conclusie en Advies\n",
    "\n",
    "## 4.1 Vergelijking van Model Prestaties\n",
    "\n",
    "De tabel hieronder laat zien hoe goed onze modellen en ensembles presteerden met validation F1-scores uit onze notebook. We hebben SMOTE en class weights gebruikt om de imbalanced dataset te fixen. De beste hyperparameters komen uit cross-validation en tuning.\n",
    "\n",
    "| Model              | Validation F1 | Beste Hyperparameters                                                                                  |\n",
    "|---------------------|---------------|---------------------------------------------------------------------------------------------------------|\n",
    "| KNN                | 0.076         | k=11, weights='distance', metric='euclidean'                                                           |\n",
    "| Logistic Regression | 0.125         | C=0.01, penalty='l1', solver='liblinear'                                                               |\n",
    "| SVM                | 0.088         | C=0.1, kernel='linear'                                                                                 |\n",
    "| Decision Tree      | 0.0997        | criterion='entropy', max_depth=5, min_samples_split=2, min_samples_leaf=1                              |\n",
    "| Random Forest      | 0.108         | criterion='gini', max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100               |\n",
    "| Gradient Boosting  | 0.106         | learning_rate=0.1, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200              |\n",
    "| XGBoost            | 0.106         | colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8                 |\n",
    "| Custom Ensemble    | 0.095         | knn_n_neighbors=9, knn_weights='distance', lr_C=10, rf_max_depth=5, rf_n_estimators=50                                                                                                       |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4.2 Beste Model\n",
    "\n",
    "Logistic Regression was het beste met een validation F1-score van 0.125. We wilden eerst de Custom Ensemble zo goed mogelijk maken, met Logistic Regression, Random Forest en SVM samen met soft voting. Maar zelfs na tuning van parameters zoals knn_n_neighbors=9 en lr_C=10, kregen we maar een F1-score van 0.095. Logistic Regression deed het beter omdat het lineaire patronen vindt, zoals hoe age met stroke samenhangt (r=0.149 uit onze EDA). De L1 regularization met C=0.01 zorgde dat het model focust op belangrijke features zoals hypertension en heart_disease, zonder te overfitten op onze imbalanced dataset. De beste parameters waren C=0.01, penalty='l1', solver='liblinear', gevonden met 5-fold cross-validation en SMOTE.\n",
    "\n",
    "## 4.3 Andere Dingen om te Overwegen\n",
    "\n",
    "Naast prestaties keken we naar andere factoren om een model te kiezen:\n",
    "\n",
    "Duidelijkheid voor Dokters: Logistic Regression is makkelijk te snappen omdat de coefficients laten zien hoe features zoals age of hypertension stroke risico beïnvloeden. Dit is handiger voor dokters dan Random Forest of XGBoost, die meer een black box zijn. We wilden vooral goede prestaties, maar dat Logistic Regression zo duidelijk is, was een bonus.\n",
    "\n",
    "Snelheid: Logistic Regression is super snel, wat goed is voor ziekenhuizen die snelle voorspellingen nodig hebben. KNN en SVM zijn traag, vooral op grote datasets. Random Forest en XGBoost zijn sneller dan KNN, maar vragen meer rekenkracht dan Logistic Regression. In het einde alle modellen doen best wel lang dus het is niet aanheraden in ze in haaste sitauties te gebruiken.\n",
    "\n",
    "Makkelijk te Gebruiken: Logistic Regression is simpel om in een ziekenhuis systeem te zetten, want het heeft niet veel resources nodig. Dit past bij onze mening dat voorspellingen niet in super urgente situaties gebruikt moeten worden, omdat de F1-scores, zoals 0.125 voor Logistic Regression, matig zijn. Dokters moeten altijd de resultaten checken.\n",
    "\n",
    "## 4.4 Advies voor Dokters\n",
    "\n",
    "Wij raden Logistic Regression aan voor dokters om stroke risico te voorspellen. Het heeft de beste F1-score van 0.125, wat precision en recall goed balanceert voor onze imbalanced dataset waar strokes zeldzaam zijn. De coefficients maken duidelijk hoe age of hypertension de voorspelling beïnvloeden, wat dokters helpt om beslissingen te maken. Maar omdat de F1-score niet super goed is en correlaties zoals age (r=0.149) zwak waren, moeten dokters de voorspellingen zien als een algemene gids, niet als een definitieve antwoord. Dokters moeten hun eigen expertise erbij gebruiken, vooral in urgente gevallen. De beste parameters zijn C=0.01, penalty='l1', solver='liblinear'.\n",
    "\n",
    "Reflectie: Het lastigste was de imbalanced dataset, met weinig stroke gevallen, wat het moeilijk maakte om goede voorspellingen te krijgen. SMOTE hielp met synthetic samples, maar het was niet genoeg voor echt betrouwbare modellen. Zwakke correlaties, zoals age met 0.149, en multicollinearity problemen, waar we sommige features moesten droppen door age, maakten het ook lastiger. In de toekomst hebben we betere data nodig om de prestaties te verbeteren."
   ],
   "id": "70079c9c926e611e"
  },
  {
   "cell_type": "markdown",
   "id": "67bc341e",
   "metadata": {},
   "source": [
    "# 5.0 Referentielijst\n",
    "\n",
    "In de .ipynb op volgorde:\n",
    "\n",
    "- ChatGPT, 2025, prompt 1: Markdown Table Help. https://chatgpt.com/share/68e0c97c-687c-8002-b0f8-10e5e1e77ccb\n",
    "- 1.6. Nearest neighbors. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/neighbors.html\n",
    "- 1.1. Linear models. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "- 1.4. Support vector machines. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/svm.html\n",
    "- Wikipedia contributors. (2025, August 13). Support vector machine. Wikipedia. https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "- 1.10. Decision Trees. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/tree.html\n",
    "- 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/ensemble.html#random-forests\n",
    "- XGBoost Documentation — xgboost 3.0.5 documentation. (n.d.). https://xgboost.readthedocs.io/en/stable/\n",
    "\n",
    "\n",
    "In aparte .py bestanden te vinden:\n",
    "- scikit-learn, 2025, make_scorer: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "- Imbalanced learning, 2025, SMOTE: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "- scikit-learn, 2025, StarfieldKFold: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "- ChatGPT, 2025, prompt 1: Export. https://chatgpt.com/share/68e262a7-8b80-800a-9b55-d7e59d1f731a\n",
    "- ChatGPT, 2025, prompt: Verbeteren van custom ensemble model: https://chatgpt.com/share/68e281a6-b384-800a-8744-210bd4b3c038"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
