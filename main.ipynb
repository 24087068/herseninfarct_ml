{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46c2472cae52391",
   "metadata": {},
   "source": [
    "# Project: Herseninfarct\n",
    "### Team: Undefined\n",
    "### Teamleden:\n",
    "- **Sebastiaan Westerlaken**\n",
    "- **Michal Kakol**"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:02.448643Z",
     "start_time": "2025-10-05T14:33:01.860716Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "# Import src\n",
    "from src.preprocessing import clean_data, normalize_data\n",
    "from src.eda import eda_sum, impossible_values\n",
    "from src.model import train_knn, train_lr, train_svm, train_decision_tree, train_random_forest, train_gradient_boosting, train_xgboost, train_custom_ensemble\n",
    "from src.other_functions import evaluate_knn, export_submission\n",
    "\n",
    "# Data\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_example = pd.read_csv(\"data/sample_submission.csv\")\n",
    "df_submission = pd.DataFrame(columns=[\"id\", \"stroke\"])"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "49f930f4",
   "metadata": {},
   "source": [
    "- ChatGPT, 2025, prompt 1/2: Markdown Table Help. https://chatgpt.com/share/68e0c97c-687c-8002-b0f8-10e5e1e77ccb\n",
    "| **Kolomnaam**                    | **Beschrijving**                                                                                |\n",
    "| -------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `id`                             | Unieke identificatiecode voor elke persoon in de dataset.                                       |\n",
    "| `age`                            | Leeftijd van de persoon (in jaren).                                                             |\n",
    "| `hypertension`                   | Of de persoon een hoge bloeddruk (hypertensie) heeft (1 = Ja, 0 = Nee).                         |\n",
    "| `heart_disease`                  | Of de persoon een hartaandoening heeft (1 = Ja, 0 = Nee).                                       |\n",
    "| `avg_glucose_level`              | Gemiddeld glucosegehalte in het bloed (in mg/dL).                                               |\n",
    "| `bmi`                            | Body Mass Index (BMI) — maat voor vetpercentage op basis van lengte en gewicht.                 |\n",
    "| `gender_Female`                  | True als de persoon **vrouw** is; anders False.                                                 |\n",
    "| `gender_Male`                    | True als de persoon **man** is; anders False.                                                   |\n",
    "| `gender_Other`                   | True als de persoon een **ander gender** heeft; anders False.                                   |\n",
    "| `ever_married_No`                | True als de persoon **nog nooit getrouwd** is; anders False.                                    |\n",
    "| `ever_married_Yes`               | True als de persoon **ooit getrouwd** is; anders False.                                         |\n",
    "| `work_type_Govt_job`             | True als de persoon een **overheidsbaan** heeft; anders False.                                  |\n",
    "| `work_type_Never_worked`         | True als de persoon **nog nooit gewerkt** heeft; anders False.                                  |\n",
    "| `work_type_Private`              | True als de persoon in de **private sector** werkt; anders False.                               |\n",
    "| `work_type_Self-employed`        | True als de persoon **zelfstandig ondernemer** is; anders False.                                |\n",
    "| `work_type_children`             | True als de persoon een **kind** is (nog geen onderdeel van de beroepsbevolking); anders False. |\n",
    "| `Residence_type_Rural`           | True als de persoon in een **landelijk** gebied woont; anders False.                            |\n",
    "| `Residence_type_Urban`           | True als de persoon in een **stedelijk** gebied woont; anders False.                            |\n",
    "| `smoking_status_formerly smoked` | True als de persoon **vroeger rookte**; anders False.                                           |\n",
    "| `smoking_status_never smoked`    | True als de persoon **nooit gerookt** heeft; anders False.                                      |\n",
    "| `smoking_status_smokes`          | True als de persoon **momenteel rookt**; anders False.                                          |\n",
    "| `stroke`                         | Doelvariabele — of de persoon ooit een **beroerte (stroke)** heeft gehad (1 = Ja, 0 = Nee).     |\n",
    "\n",
    "(ChatGPT, 2025, prompt 1/2: Markdown Table Help)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed7aec29b65d10",
   "metadata": {},
   "source": [
    "# 1. Exploratieve Data Analyse<br>\n",
    "\n",
    "Hieronder voeren wij eerst de exploratieve data analyse uit die wordt aangeroepen vanuit het src.eda bestand."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f2a7d989d469af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:29:40.241466Z",
     "start_time": "2025-10-05T14:29:40.225211Z"
    }
   },
   "source": [
    "eda_sum(df_train)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m eda_sum(df_train)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "5da4c6a6d8c6ee75",
   "metadata": {},
   "source": [
    "## Exploratieve Data Analyse\n",
    "\n",
    "### Beoordeling van de dataset\n",
    "\n",
    "De dataset kan direct gebruikt worden met Scikit-Learn. Hij is ingeladen als een DataFrame, er zijn geen ontbrekende waarden en alle kolommen zijn numeriek. Het gaat om een combinatie van booleans, integers en floats.\n",
    "\n",
    "Wat betreft de meetniveaus:\n",
    "- Nominaal: heart_disease, hypertension, work_type_Self-employed, stroke\n",
    "- Ordinaal: geen\n",
    "- Interval: geen\n",
    "- Ratio: age, avg_glucose_level, bmi\n",
    "\n",
    "Hoewel sommige nominale variabelen als integers zijn opgeslagen, zijn ze binair van aard. Voor machine learning hoeft hier niets aan veranderd te worden.\n",
    "\n",
    "### Correlaties en belangrijkste variabelen\n",
    "\n",
    "Om te kijken naar mogelijke multicollineariteit en te zien welke features het meest relevant zijn voor de target (stroke), is een correlatie-analyse gedaan.\n",
    "\n",
    "Sterkst gecorreleerde variabelen met stroke zijn:\n",
    "- age 0.149\n",
    "- heart_disease 0.105\n",
    "- hypertension 0.084\n",
    "- avg_glucose_level 0.077\n",
    "- work_type_Self-employed 0.062\n",
    "- smoking_status_formerly smoked 0.034\n",
    "- bmi 0.021\n",
    "\n",
    "Geen enkele correlatie is hoger dan 0.2, dus alle features kunnen worden behouden voor modelbouw.\n",
    "\n",
    "### Belangrijkste observaties\n",
    "\n",
    "Leeftijd blijkt duidelijk een belangrijke factor: jongere mensen hebben bijna geen kans op een beroerte, het risico neemt vooral toe na 35 jaar. BMI lijkt minder invloed te hebben dan verwacht. Voor roken valt op dat voormalige rokers iets meer risico lijken te hebben dan huidige rokers. Het gemiddeld glucosegehalte ligt hoger bij mensen met een beroerte, maar strokes komen voor bij een breed bereik aan waarden.\n",
    "\n",
    "### Statistische kenmerken\n",
    "\n",
    "Voor de niet-boolean variabelen zijn enkele kernstatistieken berekend. Age heeft een standaarddeviatie van 22.48 en is bijna symmetrisch verdeeld, met een licht afgeplatte verdeling. Avg_glucose_level ligt gemiddeld rond 104 met een standaarddeviatie van 42, de verdeling is rechts-scheef door enkele uitschieters. Deze uitschieters kunnen relevant zijn voor het model.\n",
    "\n",
    "### Klasse-ongelijkheid\n",
    "\n",
    "De dataset is sterk uit balans, met slechts 517 positieve stroke-gevallen. Daarom worden bij het evalueren van modellen vooral de F1-score en de confusion matrix gebruikt, omdat alleen accuracy niet genoeg zegt over de prestaties op de minderheidsklasse.\n",
    "\n",
    "### Samenvatting\n",
    "\n",
    "De dataset is geschikt voor machine learning. Er zijn geen missende waarden, geen sterke correlaties en de belangrijkste voorspellende variabelen lijken age, heart_disease, hypertension en avg_glucose_level. Vanwege de sterke class imbalance moet bij modeltraining extra aandacht worden besteed aan geschikte evaluatiemetrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb19032",
   "metadata": {},
   "source": [
    "## 1.1 Data opschonen en standaardiseren"
   ]
  },
  {
   "cell_type": "code",
   "id": "de445b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:31.125223Z",
     "start_time": "2025-10-05T13:59:31.084152Z"
    }
   },
   "source": [
    "# Opschonen\n",
    "df_train_clean = clean_data(df_train)\n",
    "\n",
    "# Features + target\n",
    "X = df_train_clean.drop(columns=['stroke'])\n",
    "y = df_train_clean['stroke']\n",
    "\n",
    "# Standaardiseren en align train & test automatisch\n",
    "X_scaled, X_scaled_test, scaler = normalize_data(X, df_test)\n",
    "\n",
    "X_scaled.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id       age  hypertension  heart_disease  avg_glucose_level  \\\n",
       "0  0.763423 -0.525188     -0.310285      -0.212086          -0.949341   \n",
       "1  1.700336  1.475539      3.222846      -0.212086           2.652854   \n",
       "2 -0.492650 -1.192097     -0.310285      -0.212086           0.752771   \n",
       "3  1.361419  0.230642     -0.310285      -0.212086          -0.918683   \n",
       "4  1.557020  0.319564     -0.310285      -0.212086           2.823255   \n",
       "\n",
       "        bmi  gender_Female  gender_Male  gender_Other  ever_married_No  ...  \\\n",
       "0  0.631584          False         True         False             True  ...   \n",
       "1  2.573031           True        False         False            False  ...   \n",
       "2 -1.257035          False         True         False             True  ...   \n",
       "3 -0.041979          False         True         False            False  ...   \n",
       "4  0.037263          False         True         False            False  ...   \n",
       "\n",
       "   work_type_Govt_job  work_type_Never_worked  work_type_Private  \\\n",
       "0               False                   False               True   \n",
       "1               False                   False              False   \n",
       "2               False                   False               True   \n",
       "3               False                   False              False   \n",
       "4               False                   False              False   \n",
       "\n",
       "   work_type_Self-employed  work_type_children  Residence_type_Rural  \\\n",
       "0                    False               False                  True   \n",
       "1                     True               False                  True   \n",
       "2                    False               False                  True   \n",
       "3                     True               False                 False   \n",
       "4                     True               False                  True   \n",
       "\n",
       "   Residence_type_Urban  smoking_status_formerly smoked  \\\n",
       "0                 False                           False   \n",
       "1                 False                           False   \n",
       "2                 False                           False   \n",
       "3                  True                           False   \n",
       "4                 False                            True   \n",
       "\n",
       "   smoking_status_never smoked  smoking_status_smokes  \n",
       "0                        False                   True  \n",
       "1                         True                  False  \n",
       "2                         True                  False  \n",
       "3                         True                  False  \n",
       "4                        False                  False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>ever_married_No</th>\n",
       "      <th>...</th>\n",
       "      <th>work_type_Govt_job</th>\n",
       "      <th>work_type_Never_worked</th>\n",
       "      <th>work_type_Private</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>Residence_type_Rural</th>\n",
       "      <th>Residence_type_Urban</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.763423</td>\n",
       "      <td>-0.525188</td>\n",
       "      <td>-0.310285</td>\n",
       "      <td>-0.212086</td>\n",
       "      <td>-0.949341</td>\n",
       "      <td>0.631584</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.700336</td>\n",
       "      <td>1.475539</td>\n",
       "      <td>3.222846</td>\n",
       "      <td>-0.212086</td>\n",
       "      <td>2.652854</td>\n",
       "      <td>2.573031</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.492650</td>\n",
       "      <td>-1.192097</td>\n",
       "      <td>-0.310285</td>\n",
       "      <td>-0.212086</td>\n",
       "      <td>0.752771</td>\n",
       "      <td>-1.257035</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.361419</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>-0.310285</td>\n",
       "      <td>-0.212086</td>\n",
       "      <td>-0.918683</td>\n",
       "      <td>-0.041979</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.557020</td>\n",
       "      <td>0.319564</td>\n",
       "      <td>-0.310285</td>\n",
       "      <td>-0.212086</td>\n",
       "      <td>2.823255</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "cc156ab8",
   "metadata": {},
   "source": [
    "# 2. Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "id": "d699f4caf7fad99e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:34.440794Z",
     "start_time": "2025-10-05T13:59:33.945493Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "evaluate_knn(X_train, X_test, y_train, y_test, n_neighbors=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6499   94]\n",
      " [  98    5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      6593\n",
      "           1       0.05      0.05      0.05       103\n",
      "\n",
      "    accuracy                           0.97      6696\n",
      "   macro avg       0.52      0.52      0.52      6696\n",
      "weighted avg       0.97      0.97      0.97      6696\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "955ffc11",
   "metadata": {},
   "source": [
    "### Evaluatie van het model\n",
    "\n",
    "Om het KNN-model te beoordelen gebruiken we enkele standaard metrics: accuracy, precision, recall en F1-score. Al deze waarden zijn af te leiden uit de confusion matrix, die laat zien hoe goed het model voorspellingen maakt.\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Voor het model met n_neighbors gelijk aan 1 ziet de confusion matrix er als volgt uit:\n",
    "\n",
    "Actual zonder stroke voorspeld als geen stroke: 6499\n",
    "Actual zonder stroke voorspeld als stroke: 94\n",
    "Actual met stroke voorspeld als geen stroke: 98\n",
    "Actual met stroke voorspeld als stroke: 5\n",
    "\n",
    "In termen van de standaard terminologie betekent dit:\n",
    "- True Negatives: 6499\n",
    "- False Positives: 94\n",
    "- False Negatives: 98\n",
    "- True Positives: 5\n",
    "\n",
    "### Uitleg van de metrics\n",
    "\n",
    "Accuracy meet het aandeel correcte voorspellingen, in dit geval ongeveer 97 procent. Dit lijkt hoog, maar is misleidend door de scheve verdeling van de klassen.\n",
    "\n",
    "Precision laat zien hoe vaak de voorspelde positieve gevallen echt positief zijn. Voor dit model is dat maar 0.05, dus het model voorspelt veel te vaak een stroke terwijl dat niet klopt.\n",
    "\n",
    "Recall geeft aan hoeveel van de echte strokes het model herkent. Ook dit is 0.05, wat betekent dat bijna alle strokes gemist worden.\n",
    "\n",
    "De F1-score combineert precision en recall tot één waarde die beide aspecten meeweegt. Bij scheve datasets geeft dit een realistischer beeld van het model.\n",
    "\n",
    "### Waarom F1-score belangrijk is\n",
    "\n",
    "De dataset bevat veel meer mensen zonder stroke dan met stroke. Bij zulke datasets kan een model dat altijd “geen stroke” voorspelt toch een hoge accuracy halen. Daarom is de F1-score hier een betere maatstaf. Het laat zien hoe goed het model de kleine positieve klasse oppikt en geeft een eerlijker beeld van de prestaties bij het detecteren van strokes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d3d57",
   "metadata": {},
   "source": [
    "# 3. Modelleren en uitleg modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8e922",
   "metadata": {},
   "source": [
    "## 3.1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d8e47",
   "metadata": {},
   "source": [
    "### Model 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Hoe het model werkt:\n",
    "KNN is een niet-parametrisch classificatiemodel dat een nieuwe observatie indeelt op basis van de afstand tot de dichtstbijzijnde trainingspunten. Meestal wordt de Euclidische afstand gebruikt, wat de rechte lijn tussen twee punten meet:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_i (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "Een alternatief is de Manhattan afstand, die de afstand langs de assen berekent:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sum_i |p_i - q_i|\n",
    "$$\n",
    "\n",
    "Het model kiest de klasse van het nieuwe punt op basis van de meerderheid van de $k$ dichtstbijzijnde buren.\n",
    "\n",
    "Waarom standaardisatie belangrijk is:\n",
    "Omdat KNN afstanden vergelijkt tussen features, moeten alle features op dezelfde schaal staan. Zonder standaardisatie kunnen features met grotere waarden, zoals inkomen, de afstand te veel domineren vergeleken met kleinere waarden zoals leeftijd. Dit kan de voorspellingen van het model vervormen.\n",
    "\n",
    "Beste hyperparameters:\n",
    "Voor dit model zijn gekozen: metric = euclidean, k = 11 en weights = distance. Met deze instellingen behaalt het model een F1-score van 0.076.\n",
    "\n",
    "Regularisatie bij KNN:\n",
    "Regularisatie gebeurt hier indirect via de parameter $k$.\n",
    "- Een kleine $k$ betekent dat het model sterk reageert op individuele datapunten en kan overfitten.\n",
    "- Een grotere $k$ maakt het model gladder en helpt bij generalisatie.\n",
    "\n",
    "Daarnaast zorgt weights = distance ervoor dat buren die dichterbij liggen meer invloed hebben op de voorspelling, wat zorgt voor een stabieler resultaat."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9149aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T13:59:43.343890Z",
     "start_time": "2025-10-05T13:59:37.013228Z"
    }
   },
   "source": [
    "knn_model, knn_params, knn_f1 = train_knn(X_scaled, y)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m knn_model, knn_params, knn_f1 \u001B[38;5;241m=\u001B[39m train_knn(X_scaled, y)\n",
      "File \u001B[1;32m~\\Portables\\Projects\\herseninfarct_ml\\src\\model.py:46\u001B[0m, in \u001B[0;36mtrain_knn\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m     38\u001B[0m scorer \u001B[38;5;241m=\u001B[39m make_scorer(f1_score)\n\u001B[0;32m     39\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m     40\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mpipeline,\n\u001B[0;32m     41\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     44\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     45\u001B[0m )\n\u001B[1;32m---> 46\u001B[0m grid\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m     47\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[0;32m     48\u001B[0m best_params \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1020\u001B[0m     )\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1024\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1570\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1571\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    967\u001B[0m         )\n\u001B[0;32m    968\u001B[0m     )\n\u001B[1;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    971\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    972\u001B[0m         clone(base_estimator),\n\u001B[0;32m    973\u001B[0m         X,\n\u001B[0;32m    974\u001B[0m         y,\n\u001B[0;32m    975\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    976\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    977\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    978\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    979\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    980\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    981\u001B[0m     )\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    983\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    985\u001B[0m     )\n\u001B[0;32m    986\u001B[0m )\n\u001B[0;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    993\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1689c1569da6ccb1",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f23f4046cf494c",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression\n",
    "\n",
    "Hoe het model werkt:\n",
    "Logistische regressie voorspelt de kans dat een observatie tot klasse 1 behoort met behulp van de sigmoidfunctie:\n",
    "\n",
    "$$\n",
    "P(y=1 | x) = \\frac{1}{1 + e^{-(w^T x + b)}}\n",
    "$$\n",
    "\n",
    "Als deze kans groter is dan 0.5 voorspelt het model klasse 1, anders klasse 0.\n",
    "\n",
    "Loss-functie:\n",
    "Om te meten hoe goed de voorspellingen zijn wordt de log-loss gebruikt, ook bekend als binaire cross-entropy:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "Gradient Descent:\n",
    "Het model past de gewichten aan door de loss te minimaliseren met gradient descent:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Hierbij is α de learning rate. Bijvoorbeeld, als de gradient 4 is en α 0.1, wordt w aangepast met 0.4 richting het minimum van de loss.\n",
    "\n",
    "Regularisatie:\n",
    "Regularisatie helpt overfitting te voorkomen door grote coëfficiënten te straffen.\n",
    "\n",
    "- L1 (Lasso) kan sommige gewichten nul maken en werkt zo ook als feature selectie.\n",
    "- L2 (Ridge) maakt gewichten kleiner zonder ze nul te maken, waardoor het model beter generaliseert.\n",
    "\n",
    "Beste hyperparameters:\n",
    "C = 100, penalty = l2, solver = liblinear.\n",
    "Met deze instellingen behaalt het model een F1-score van 0.105.\n",
    "\n",
    "Regularisatie en overfitting:\n",
    "Hoge gewichten laten zien dat het model sterk afhankelijk is van bepaalde features, wat overfitting kan veroorzaken. Door regularisatie worden de gewichten verlaagd, wat helpt om het model beter te laten generaliseren."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5601e157a04add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:20:34.122393Z",
     "start_time": "2025-10-05T11:19:24.177904Z"
    }
   },
   "source": [
    "lr_model, lr_params, lr_f1 = train_lr(X_scaled, y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'lr__C': 100, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n",
      "Best F1 score: 0.10512810274749802\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "5cc0af4de2e6e8fb",
   "metadata": {},
   "source": [
    "## 3.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5f9d1",
   "metadata": {},
   "source": [
    "### Model 3: Support Vector Machine (SVM)\n",
    "\n",
    "Hoe het model werkt:\n",
    "SVM is een classificatiemodel dat een optimale scheidingslijn, of hypervlak, zoekt om de twee klassen van elkaar te scheiden met de grootst mogelijke marge:\n",
    "\n",
    "$$\n",
    "\\text{maximize } \\frac{2}{||w||} \\text{ onder de constraint } y_i (w^T x_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "Hierbij zijn:\n",
    "- \\(w\\) de gewichten, \\(b\\) de bias\n",
    "- \\(x_i\\) de featurevector van observatie \\(i\\)\n",
    "- \\(y_i \\in \\{-1, 1\\}\\) de klasse van observatie \\(i\\)\n",
    "\n",
    "Kernel en Kernel Trick:\n",
    "Wanneer data niet lineair scheidbaar zijn, kan een kernel \\(K(x_i, x_j)\\) de data transformeren naar een hogere dimensie:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
    "$$\n",
    "\n",
    "De kernel trick maakt het mogelijk \\(\\phi(x)\\) niet expliciet te berekenen, waardoor het model efficiënter blijft. Bijvoorbeeld, met twee features \\((x_1, x_2)\\) en een linear kernel:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = x_i^T x_j = x_{i1} x_{j1} + x_{i2} x_{j2}\n",
    "$$\n",
    "\n",
    "Loss-functie:\n",
    "SVM gebruikt hinge loss:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\max(0, 1 - y \\hat{y})\n",
    "$$\n",
    "\n",
    "Met \\(\\hat{y} = w^T x + b\\). Het verlies is nul als een voorbeeld correct is geclassificeerd met marge groter dan of gelijk aan 1, anders neemt het lineair toe.\n",
    "\n",
    "Regularisatie:\n",
    "De C-parameter reguleert de balans tussen correcte classificatie en maximale marge.\n",
    "- Hoger C betekent minder foutacceptatie\n",
    "- Lager C laat een grotere marge toe\n",
    "\n",
    "De totale loss met regularisatie kan worden weergegeven als:\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{1}{2} ||w||^2 + C \\sum_i \\max(0, 1 - y_i (w^T x_i + b))\n",
    "$$\n",
    "\n",
    "Beste hyperparameters:\n",
    "- C = 0.1\n",
    "- kernel = linear\n",
    "- gamma = scale\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.1035.\n",
    "\n",
    "Gradient / Optimalisatie:\n",
    "SVM optimaliseert de gewichten \\(w\\) via convex optimization en gradient-based methodes, zodat de som van de hinge loss en de regularisatieterm minimaal wordt."
   ]
  },
  {
   "cell_type": "code",
   "id": "8aa653e6fef813da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:42:43.275355Z",
     "start_time": "2025-10-05T11:20:34.322981Z"
    }
   },
   "source": [
    "svm_model, svm_params, svm_f1 = train_svm(X_scaled, y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'svm__C': 0.1, 'svm__degree': 2, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Best F1 score: 0.10351940940691495\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "badab96adc5c714b",
   "metadata": {},
   "source": [
    "## 3.4 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65ed59",
   "metadata": {},
   "source": [
    "### Model 4: Decision Tree\n",
    "\n",
    "Hoe het model werkt:\n",
    "Decision Tree splitst de data op basis van beslissingen over features. Elke splitsing verdeelt de data in twee takken totdat een stopcriterium is bereikt, zoals een maximale diepte of minimaal aantal voorbeelden in een blad.\n",
    "\n",
    "Splitsingscriteria:\n",
    "De splitsing wordt gekozen op basis van het criterium dat de impurity of onzekerheid het meest vermindert. Twee veelgebruikte criteria zijn:\n",
    "\n",
    "- Gini impurity:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "- Entropy of Information Gain:\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Hierbij is \\(p_i\\) de kans dat een voorbeeld tot klasse \\(i\\) behoort.\n",
    "\n",
    "Regularisatie en Pruning:\n",
    "Om overfitting te voorkomen kunnen verschillende parameters worden gebruikt:\n",
    "- max_depth: maximale boomdiepte\n",
    "- min_samples_split: minimaal aantal voorbeelden nodig om een knoop te splitsen\n",
    "- min_samples_leaf: minimaal aantal voorbeelden per blad\n",
    "- Pruning: takken die weinig bijdragen worden verwijderd\n",
    "\n",
    "Pruning vermindert overfitting, maar te veel pruning kan juist leiden tot underfitting.\n",
    "\n",
    "Beste hyperparameters:\n",
    "- criterion: gini\n",
    "- max_depth: 5\n",
    "- min_samples_split: 2\n",
    "- min_samples_leaf: 4\n",
    "\n",
    "Met deze instellingen behaalt het model een F1-score van 0.0854."
   ]
  },
  {
   "cell_type": "code",
   "id": "5da03b5e125ae635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:43:07.006607Z",
     "start_time": "2025-10-05T11:42:43.456644Z"
    }
   },
   "source": [
    "dt_model, dt_params, dt_f1 = train_decision_tree(X_scaled, y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'dt__criterion': 'gini', 'dt__max_depth': 5, 'dt__min_samples_leaf': 4, 'dt__min_samples_split': 2}\n",
      "Best F1 score: 0.0854327793718217\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "e3ecd63a670b4b44",
   "metadata": {},
   "source": [
    "## 3.5 Ensembles (Random Forest, Gradient Boosted Boosted Decistion Trees en XGboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f62468",
   "metadata": {},
   "source": [
    "### Ensembles: Random Forest, Gradient Boosted Trees en XGBoost\n",
    "\n",
    "Random Forest (RF):\n",
    "Random Forest combineert meerdere beslisbomen via bagging, oftewel bootstrap aggregation. Elke boom wordt onafhankelijk getraind, waardoor training relatief snel kan verlopen en parallel uitgevoerd wordt. Belangrijke hyperparameters zijn n_estimators, max_depth, min_samples_split, min_samples_leaf en criterion. De beste instellingen voor dit model zijn criterion = gini, max_depth = 10, min_samples_leaf = 2, min_samples_split = 2 en n_estimators = 100. Met deze instellingen behaalt het model een F1-score van 0.0975.\n",
    "\n",
    "Gradient Boosted Trees (GBT):\n",
    "GBT traint meerdere bomen achter elkaar met boosting. Elke nieuwe boom corrigeert de fouten van de vorige bomen. Dit geeft vaak betere prestaties, maar de training duurt langer omdat het sequential is. Belangrijke hyperparameters zijn n_estimators, learning_rate, max_depth, min_samples_split en min_samples_leaf. De beste instellingen zijn learning_rate = 0.1, max_depth = 3, min_samples_leaf = 2, min_samples_split = 2 en n_estimators = 100. Dit levert een F1-score van 0.1011.\n",
    "\n",
    "XGBoost:\n",
    "XGBoost is een geavanceerde vorm van boosting, die door optimalisaties zoals parallel split search sneller kan trainen. Extra hyperparameters zoals subsample en colsample_bytree helpen bij regularisatie en verminderen variance. Belangrijke hyperparameters zijn n_estimators, learning_rate, max_depth, subsample en colsample_bytree. De beste instellingen zijn colsample_bytree = 0.8, learning_rate = 0.1, max_depth = 3, n_estimators = 100 en subsample = 1.0. Dit model behaalt een F1-score van 0.1016."
   ]
  },
  {
   "cell_type": "code",
   "id": "8313ab4ab7adbddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:59:08.539550Z",
     "start_time": "2025-10-05T12:59:03.148797Z"
    }
   },
   "source": [
    "rf_model, rf_params, rf_f1 = train_random_forest(X_scaled, y)\n",
    "gb_model, gb_params, gb_f1 = train_gradient_boosting(X_scaled, y)\n",
    "xg_model, xg_params, xg_f1 = train_xgboost(X_scaled, y)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m rf_model, rf_params, rf_f1 \u001B[38;5;241m=\u001B[39m train_random_forest(X_scaled, y)\n\u001B[0;32m      2\u001B[0m gb_model, gb_params, gb_f1 \u001B[38;5;241m=\u001B[39m train_gradient_boosting(X_scaled, y)\n\u001B[0;32m      3\u001B[0m xg_model, xg_params, xg_f1 \u001B[38;5;241m=\u001B[39m train_xgboost(X_scaled, y)\n",
      "File \u001B[1;32m~\\Portables\\Projects\\herseninfarct_ml\\src\\model.py:189\u001B[0m, in \u001B[0;36mtrain_random_forest\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m    187\u001B[0m scorer \u001B[38;5;241m=\u001B[39m make_scorer(f1_score)\n\u001B[0;32m    188\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(pipeline, param_grid, scoring\u001B[38;5;241m=\u001B[39mscorer, cv\u001B[38;5;241m=\u001B[39mcv, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 189\u001B[0m grid\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m    190\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[0;32m    191\u001B[0m best_params \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1020\u001B[0m     )\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1024\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1570\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1571\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    967\u001B[0m         )\n\u001B[0;32m    968\u001B[0m     )\n\u001B[1;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    971\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    972\u001B[0m         clone(base_estimator),\n\u001B[0;32m    973\u001B[0m         X,\n\u001B[0;32m    974\u001B[0m         y,\n\u001B[0;32m    975\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    976\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    977\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    978\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    979\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    980\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    981\u001B[0m     )\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    983\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    985\u001B[0m     )\n\u001B[0;32m    986\u001B[0m )\n\u001B[0;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    993\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "3c16d04c6ac78245",
   "metadata": {},
   "source": [
    "## 3.6 Zelf samengesteld ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c78eb",
   "metadata": {},
   "source": [
    "### Custom Ensemble Model\n",
    "\n",
    "Werking van het ensemble:\n",
    "Het ensemble combineert minimaal drie verschillende modellen: KNN, Logistic Regression en Random Forest. Hierbij wordt soft voting gebruikt, wat betekent dat de voorspelde kansen van de basismodellen gemiddeld worden. De klasse met de hoogste gemiddelde kans wordt gekozen als uiteindelijke voorspelling.\n",
    "\n",
    "Keuze van modellen:\n",
    "- KNN: afstand-gebaseerd en gevoelig voor de schaal van features.\n",
    "- Logistic Regression: lineair en kans-gebaseerd.\n",
    "- Random Forest: non-lineair, robuust tegen outliers en kan interacties tussen features oppikken.\n",
    "\n",
    "Beste hyperparameters:\n",
    "- knn_n_neighbors = 9\n",
    "- knn_weights = distance\n",
    "- lr_C = 10\n",
    "- rf_max_depth = 5\n",
    "- rf_n_estimators = 50\n",
    "\n",
    "Met deze instellingen behaalt het ensemble een F1-score van 0.095.\n",
    "\n",
    "Waarom dit ensemble werkt:\n",
    "Door verschillende modellen te combineren verminderen we fouten die individuele modellen maken. Dit helpt vooral bij een dataset met veel meer negatieve dan positieve gevallen, omdat sommige modellen beter presteren bij het herkennen van de zeldzame positieve klasse."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae7a7ccaf0ffa7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:28:58.491213Z",
     "start_time": "2025-10-05T13:59:49.349786Z"
    }
   },
   "source": [
    "custom_model, custom_params, custom_f1 = train_custom_ensemble(X_scaled, y)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m custom_model, custom_params, custom_f1 \u001B[38;5;241m=\u001B[39m train_custom_ensemble(X_scaled, y)\n",
      "File \u001B[1;32m~\\Portables\\Projects\\herseninfarct_ml\\src\\model.py:300\u001B[0m, in \u001B[0;36mtrain_custom_ensemble\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m    292\u001B[0m scorer \u001B[38;5;241m=\u001B[39m make_scorer(f1_score)\n\u001B[0;32m    293\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m    294\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mpipeline,\n\u001B[0;32m    295\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    298\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    299\u001B[0m )\n\u001B[1;32m--> 300\u001B[0m grid\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m    301\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[0;32m    302\u001B[0m best_params \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1020\u001B[0m     )\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1024\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1570\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1571\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    967\u001B[0m         )\n\u001B[0;32m    968\u001B[0m     )\n\u001B[1;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    971\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    972\u001B[0m         clone(base_estimator),\n\u001B[0;32m    973\u001B[0m         X,\n\u001B[0;32m    974\u001B[0m         y,\n\u001B[0;32m    975\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    976\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    977\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    978\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    979\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    980\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    981\u001B[0m     )\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    983\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    985\u001B[0m     )\n\u001B[0;32m    986\u001B[0m )\n\u001B[0;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    993\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "443c40a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:20:00.659963Z",
     "start_time": "2025-10-05T12:19:56.783553Z"
    }
   },
   "source": [
    "#Export\n",
    "# - ChatGPT, 2025, prompt 1: Export. https://chatgpt.com/share/68e262a7-8b80-800a-9b55-d7e59d1f731a\n",
    "test_ids = df_test['id']\n",
    "predictions = custom_model.predict(X_scaled_test)\n",
    "df_submission['id'] = test_ids\n",
    "df_submission['stroke'] = predictions\n",
    "\n",
    "df_submission.to_csv('data/submission_custom_ensemble.csv', index=False)\n",
    "#print(df_submission.describe())\n",
    "\n",
    "test_ids = df_test['id']\n",
    "\n",
    "trained_models = {\n",
    "    'knn': knn_model,\n",
    "    'logreg': lr_model,\n",
    "    'svm': svm_model,\n",
    "    'decision_tree': dt_model,\n",
    "    'random_forest': rf_model,\n",
    "    'gradient_boosting': gb_model,\n",
    "    'xgboost': xg_model,\n",
    "    'custom_ensemble': custom_model\n",
    "}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'data/submission_{name}_{timestamp}.csv'\n",
    "    export_submission(model, X_scaled_test, test_ids, filename)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id       stroke\n",
      "count   8388.000000  8388.000000\n",
      "mean   37094.643777     0.101335\n",
      "std    20955.196907     0.301790\n",
      "min        4.000000     0.000000\n",
      "25%    18877.750000     0.000000\n",
      "50%    37490.000000     0.000000\n",
      "75%    55290.250000     0.000000\n",
      "max    72934.000000     1.000000\n",
      "Submission saved as data/submission_knn_20251005_141957.csv\n",
      "Submission saved as data/submission_logreg_20251005_141957.csv\n",
      "Submission saved as data/submission_svm_20251005_141957.csv\n",
      "Submission saved as data/submission_decision_tree_20251005_142000.csv\n",
      "Submission saved as data/submission_random_forest_20251005_142000.csv\n",
      "Submission saved as data/submission_gradient_boosting_20251005_142000.csv\n",
      "Submission saved as data/submission_xgboost_20251005_142000.csv\n",
      "Submission saved as data/submission_custom_ensemble_20251005_142000.csv\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "67bc341e",
   "metadata": {},
   "source": [
    "# 4.0 Referentielijst\n",
    "- scikit-learn, 2025, make_scorer: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "- Imbalanced learning, 2025, SMOTE: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "- scikit-learn, 2025, StarfieldKFold: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "- ChatGPT, 2025, prompt 1/2: Markdown Table Help. https://chatgpt.com/share/68e0c97c-687c-8002-b0f8-10e5e1e77ccb\n",
    "- ChatGPT, 2025, prompt 1: Export. https://chatgpt.com/share/68e262a7-8b80-800a-9b55-d7e59d1f731a\n",
    "- ChatGPT, 2025, prompt: Verbeteren van custom ensemble model: https://chatgpt.com/share/68e281a6-b384-800a-8744-210bd4b3c038"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
